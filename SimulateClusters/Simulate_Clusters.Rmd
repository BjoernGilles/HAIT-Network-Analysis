# Script for the simulation of different clustering solutions

# Load packages
```{r}
library(pacman)
p_load(
  tidyverse,
  bibliometrix,
  psych,
  stringr,
  igraph
)
```


# Load data
Load the workspace created in the network analysis script. 

```{r}
load("network_analysis_workspace.RData")
# To skip the calculating time, instead load the workspace with the calculated solution:
# load("simulate_clusters_workspace.RData")
```

# Simulate Solutions

```{r}
# This loop calculates 10,000 new clustering solutions for the generated network.
# Results are saved each iteration and analyzed afterwards.

selected <- c()
n_clust <- c()
for (x in 1:10000) {
  # Find communities in the graph
  comm_content <- communities(multilevel.community(g2))
  
  # Filter communities with more than 20 nodes
  comm_content <- comm_content[which((lapply(comm_content, length) %>% unlist()) > 20)]
  
  subs <- list()
  
  # Create subgraphs for each community
  for (i in 1:length(comm_content)) {
    subs <- append(subs, subgraph(g2, comm_content[i] %>% unlist()) %>% list())
  }
  
  # Determine the number of papers to select from each community
  n_paper <- (mapply(comm_content, FUN = length) * 0.10) %>% round()
  
  rel_papers <- list()
  
  # Select relevant papers with top 10% Edge-Strength from each subgraph
  for (i in 1:length(subs)) {
    degrees <- strength(subs[[i]])[strength(subs[[i]]) %>% order(decreasing = TRUE)]
    papers <- degrees[1:n_paper[i]] %>% names() %>% list()
    rel_papers <- append(rel_papers, papers)
  }
  
  rel_papers_total <- strength(g2)[(strength(g2) %>% order(decreasing = TRUE))[1:30]] %>% names() %>% list()
  
  rel_papers <- append(rel_papers, rel_papers_total)
  
  rel_papers <- unlist(rel_papers) %>% table() %>% names()
  
  selected <- append(selected, rel_papers)
  
  n_clust <- append(n_clust, length(subs))
}

# How many papers are on average included using the selection criteria: top 10% strength in each cluster?

n_selected <- length(selected) / 10000

# On average 42 articles are included for the content analysis.


# How likely is it that the most commonly for review included papers are included, given differences in clusterin solutions? 
prob_table <- (table(selected) / 100)
oders <- order(prob_table, decreasing = TRUE)
sortet_table <- prob_table[oders][1:42]

# Of the 42 Papers, only 6 are included with a lower probability than 95% 

# How likely are different clustering solutions?
table(n_clust) / 100

# Probability of different clustering solutions: 3 Clusters: 0.1%, 4 Clusters: 55.7 %, 5 Clusters: 41.5%, 6 Clusters: 2.7%
```

